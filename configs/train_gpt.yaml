# Training configuration for GPT model
io:
  out_dir: "out-gpt"
  eval_interval: 500
  log_interval: 10
  eval_iters: 200
  always_save_checkpoint: true
  init_from: "resume"

wandb:
  enabled: false
  project: "popgpt"
  run_name: "gpt"

data:
  dataset: "gpt"
  batch_size: 64
  block_size: 64

model:
  n_layer: 24
  d_model: 1024
  d_head: 16
  dropout: 0.0
  bias: false

optimizer:
  learning_rate: 2.5e-4
  max_iters: 60000
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  gradient_accumulation_steps: 2

lr_schedule:
  decay_lr: true
  warmup_iters: 200
  lr_decay_iters: 10000
  min_lr: 5e-5

system:
  device: "cuda"
  dtype: "float32"
  compile: false
  seed: 1337

